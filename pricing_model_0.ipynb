{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "92849e33-af1b-4f49-9c84-cae4a72a6bcf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchmetrics\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "torch.set_default_device(device)\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a6ef7e05-2ed1-4d56-a77a-48558c0f6199",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import requests\n",
    "# from pathlib import Path \n",
    "\n",
    "# # Download helper functions from Learn PyTorch repo (if not already downloaded)\n",
    "# if Path(\"helper_functions.py\").is_file():\n",
    "#   print(\"helper_functions.py already exists, skipping download\")\n",
    "# else:\n",
    "#   print(\"Downloading helper_functions.py\")\n",
    "#   # Note: you need the \"raw\" GitHub URL for this to work\n",
    "#   request = requests.get(\"https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/helper_functions.py\")\n",
    "#   with open(\"helper_functions.py\", \"wb\") as f:\n",
    "#     f.write(request.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "29e86e34-5e17-4cb3-baab-18b9912576ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: tqdm in /home/prometheus/.local/lib/python3.10/site-packages (4.66.1)\n"
     ]
    }
   ],
   "source": [
    "!pip3 install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5bd8c65b-3a1a-4366-bc92-d56e53b6b9fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_15873/2305619842.py:1: DtypeWarning: Columns (7) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv('baseline_data.csv')\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('baseline_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6e9a886d-6301-4a6e-80c1-c40693135b66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mrd_type</th>\n",
       "      <th>original_list_price</th>\n",
       "      <th>list_price</th>\n",
       "      <th>close_price</th>\n",
       "      <th>association_fee</th>\n",
       "      <th>tax_annual_amount</th>\n",
       "      <th>days_on_market</th>\n",
       "      <th>postal_code</th>\n",
       "      <th>rooms_total</th>\n",
       "      <th>bedrooms_total</th>\n",
       "      <th>...</th>\n",
       "      <th>garage_spaces</th>\n",
       "      <th>lot_size_acres</th>\n",
       "      <th>living_area</th>\n",
       "      <th>year_built</th>\n",
       "      <th>public_remarks</th>\n",
       "      <th>elementary_school_district</th>\n",
       "      <th>middle_or_junior_school_district</th>\n",
       "      <th>high_school_district</th>\n",
       "      <th>waterfront_yn</th>\n",
       "      <th>street_dir_prefix</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Detached Single</td>\n",
       "      <td>189900.0</td>\n",
       "      <td>189900.0</td>\n",
       "      <td>190000.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1102.0</td>\n",
       "      <td>3</td>\n",
       "      <td>46366</td>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1886</td>\n",
       "      <td>1875</td>\n",
       "      <td>Perfectly situated on 10 acres of land, this 3...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>W</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Attached Single</td>\n",
       "      <td>2992002.0</td>\n",
       "      <td>2992002.0</td>\n",
       "      <td>2749916.0</td>\n",
       "      <td>1619.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>26</td>\n",
       "      <td>60601</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2586</td>\n",
       "      <td>0</td>\n",
       "      <td>Introducing Vista Residences Chicago's New Lux...</td>\n",
       "      <td>299</td>\n",
       "      <td>299</td>\n",
       "      <td>299</td>\n",
       "      <td>True</td>\n",
       "      <td>E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Attached Single</td>\n",
       "      <td>1969100.0</td>\n",
       "      <td>1969100.0</td>\n",
       "      <td>1670267.0</td>\n",
       "      <td>1064.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>60601</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2804</td>\n",
       "      <td>0</td>\n",
       "      <td>Introducing Vista Residences Chicago's New Lux...</td>\n",
       "      <td>299</td>\n",
       "      <td>299</td>\n",
       "      <td>299</td>\n",
       "      <td>True</td>\n",
       "      <td>E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Attached Single</td>\n",
       "      <td>1164240.0</td>\n",
       "      <td>1164240.0</td>\n",
       "      <td>1085145.0</td>\n",
       "      <td>633.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>60601</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1176</td>\n",
       "      <td>0</td>\n",
       "      <td>Introducing Vista Residences Chicago's New Lux...</td>\n",
       "      <td>299</td>\n",
       "      <td>299</td>\n",
       "      <td>299</td>\n",
       "      <td>True</td>\n",
       "      <td>E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Attached Single</td>\n",
       "      <td>1380000.0</td>\n",
       "      <td>1470670.0</td>\n",
       "      <td>1306059.0</td>\n",
       "      <td>750.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5</td>\n",
       "      <td>60601</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2000</td>\n",
       "      <td>0</td>\n",
       "      <td>Introducing Vista Residences Chicago's New Lux...</td>\n",
       "      <td>299</td>\n",
       "      <td>299</td>\n",
       "      <td>299</td>\n",
       "      <td>True</td>\n",
       "      <td>E</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          mrd_type  original_list_price  list_price  close_price  \\\n",
       "0  Detached Single             189900.0    189900.0     190000.0   \n",
       "1  Attached Single            2992002.0   2992002.0    2749916.0   \n",
       "2  Attached Single            1969100.0   1969100.0    1670267.0   \n",
       "3  Attached Single            1164240.0   1164240.0    1085145.0   \n",
       "4  Attached Single            1380000.0   1470670.0    1306059.0   \n",
       "\n",
       "   association_fee  tax_annual_amount  days_on_market postal_code  \\\n",
       "0              0.0             1102.0               3       46366   \n",
       "1           1619.0                0.0              26       60601   \n",
       "2           1064.0                0.0               2       60601   \n",
       "3            633.0                0.0               2       60601   \n",
       "4            750.0                0.0               5       60601   \n",
       "\n",
       "   rooms_total  bedrooms_total  ...  garage_spaces  lot_size_acres  \\\n",
       "0            9               3  ...            2.0             0.0   \n",
       "1            7               3  ...            1.0             0.0   \n",
       "2            6               3  ...            1.0             0.0   \n",
       "3            4               1  ...            1.0             0.0   \n",
       "4            7               2  ...            1.0             0.0   \n",
       "\n",
       "   living_area  year_built                                     public_remarks  \\\n",
       "0         1886        1875  Perfectly situated on 10 acres of land, this 3...   \n",
       "1         2586           0  Introducing Vista Residences Chicago's New Lux...   \n",
       "2         2804           0  Introducing Vista Residences Chicago's New Lux...   \n",
       "3         1176           0  Introducing Vista Residences Chicago's New Lux...   \n",
       "4         2000           0  Introducing Vista Residences Chicago's New Lux...   \n",
       "\n",
       "   elementary_school_district middle_or_junior_school_district  \\\n",
       "0                         NaN                              NaN   \n",
       "1                         299                              299   \n",
       "2                         299                              299   \n",
       "3                         299                              299   \n",
       "4                         299                              299   \n",
       "\n",
       "  high_school_district waterfront_yn street_dir_prefix  \n",
       "0                  NaN         False                 W  \n",
       "1                  299          True                 E  \n",
       "2                  299          True                 E  \n",
       "3                  299          True                 E  \n",
       "4                  299          True                 E  \n",
       "\n",
       "[5 rows x 22 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "391fce4d-221c-42c7-b48a-6298c259d356",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 400918 entries, 0 to 400917\n",
      "Data columns (total 22 columns):\n",
      " #   Column                            Non-Null Count   Dtype  \n",
      "---  ------                            --------------   -----  \n",
      " 0   mrd_type                          400918 non-null  object \n",
      " 1   original_list_price               400918 non-null  float64\n",
      " 2   list_price                        400918 non-null  float64\n",
      " 3   close_price                       400918 non-null  float64\n",
      " 4   association_fee                   400918 non-null  float64\n",
      " 5   tax_annual_amount                 400918 non-null  float64\n",
      " 6   days_on_market                    400918 non-null  int64  \n",
      " 7   postal_code                       400918 non-null  object \n",
      " 8   rooms_total                       400918 non-null  int64  \n",
      " 9   bedrooms_total                    400918 non-null  int64  \n",
      " 10  bathrooms_full                    400918 non-null  int64  \n",
      " 11  bathrooms_half                    400918 non-null  int64  \n",
      " 12  garage_spaces                     400918 non-null  float64\n",
      " 13  lot_size_acres                    400918 non-null  float64\n",
      " 14  living_area                       400918 non-null  int64  \n",
      " 15  year_built                        400918 non-null  int64  \n",
      " 16  public_remarks                    393372 non-null  object \n",
      " 17  elementary_school_district        398787 non-null  object \n",
      " 18  middle_or_junior_school_district  398752 non-null  object \n",
      " 19  high_school_district              398663 non-null  object \n",
      " 20  waterfront_yn                     400918 non-null  bool   \n",
      " 21  street_dir_prefix                 170768 non-null  object \n",
      "dtypes: bool(1), float64(7), int64(7), object(7)\n",
      "memory usage: 64.6+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()\n",
    "df = df.drop(['street_dir_prefix'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dd4bac2d-2c2d-433b-8d6f-1e3730562743",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['mrd_type', 'original_list_price', 'list_price', 'close_price',\n",
       "       'association_fee', 'tax_annual_amount', 'days_on_market', 'postal_code',\n",
       "       'rooms_total', 'bedrooms_total', 'bathrooms_full', 'bathrooms_half',\n",
       "       'garage_spaces', 'lot_size_acres', 'living_area', 'year_built',\n",
       "       'public_remarks', 'elementary_school_district',\n",
       "       'middle_or_junior_school_district', 'high_school_district',\n",
       "       'waterfront_yn'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "93327c38-ebc3-4d20-8e97-4540ac4b1c48",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_elementary = df[[\"postal_code\",\"elementary_school_district\"]].groupby([\"postal_code\",\"elementary_school_district\"]).size().reset_index().rename(columns={0:'count'})\n",
    "unique_middle = df[[\"postal_code\",\"middle_or_junior_school_district\"]].groupby([\"postal_code\",\"middle_or_junior_school_district\"]).size().reset_index().rename(columns={0:'count'})\n",
    "unique_high = df[[\"postal_code\",\"high_school_district\"]].groupby([\"postal_code\",\"high_school_district\"]).size().reset_index().rename(columns={0:'count'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fc40e676-ceeb-4401-87be-c650124010fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "el_dict = dict(zip(unique_elementary.postal_code, unique_elementary.elementary_school_district))\n",
    "md_dict = dict(zip(unique_middle.postal_code, unique_middle.middle_or_junior_school_district))\n",
    "hs_dict = dict(zip(unique_high.postal_code, unique_high.high_school_district))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bff422b2-88c4-48c5-9f77-a93b8bec93d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.public_remarks = df.public_remarks.fillna('')\n",
    "df.elementary_school_district = df.elementary_school_district.fillna(df.postal_code.map(el_dict))\n",
    "df.middle_or_junior_school_district = df.middle_or_junior_school_district.fillna(df.postal_code.map(md_dict))\n",
    "df.high_school_district = df.high_school_district.fillna(df.postal_code.map(hs_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6409d2a2-6a61-4f53-a6ca-422b7f16d690",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "50c077b9-e357-43a9-939f-1d71141e356f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 400040 entries, 1 to 400917\n",
      "Data columns (total 21 columns):\n",
      " #   Column                            Non-Null Count   Dtype  \n",
      "---  ------                            --------------   -----  \n",
      " 0   mrd_type                          400040 non-null  object \n",
      " 1   original_list_price               400040 non-null  float64\n",
      " 2   list_price                        400040 non-null  float64\n",
      " 3   close_price                       400040 non-null  float64\n",
      " 4   association_fee                   400040 non-null  float64\n",
      " 5   tax_annual_amount                 400040 non-null  float64\n",
      " 6   days_on_market                    400040 non-null  int64  \n",
      " 7   postal_code                       400040 non-null  object \n",
      " 8   rooms_total                       400040 non-null  int64  \n",
      " 9   bedrooms_total                    400040 non-null  int64  \n",
      " 10  bathrooms_full                    400040 non-null  int64  \n",
      " 11  bathrooms_half                    400040 non-null  int64  \n",
      " 12  garage_spaces                     400040 non-null  float64\n",
      " 13  lot_size_acres                    400040 non-null  float64\n",
      " 14  living_area                       400040 non-null  int64  \n",
      " 15  year_built                        400040 non-null  int64  \n",
      " 16  public_remarks                    400040 non-null  object \n",
      " 17  elementary_school_district        400040 non-null  object \n",
      " 18  middle_or_junior_school_district  400040 non-null  object \n",
      " 19  high_school_district              400040 non-null  object \n",
      " 20  waterfront_yn                     400040 non-null  bool   \n",
      "dtypes: bool(1), float64(7), int64(7), object(6)\n",
      "memory usage: 64.5+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "627865b0-6f84-4fb4-ac17-6b8faf958506",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_cols = list(df.select_dtypes(include=['object','bool']).columns)\n",
    "num_cols = list(df.select_dtypes(include=['float64','int64']).columns)\n",
    "\n",
    "cat_cols.remove('public_remarks',)\n",
    "num_cols.remove('close_price')\n",
    "\n",
    "df_text = df[['public_remarks']].reset_index(drop='True').astype('string')\n",
    "df_target = df[['close_price']].reset_index(drop='True')\n",
    "\n",
    "df_cat = df[cat_cols].reset_index(drop='True').astype('string')\n",
    "\n",
    "df_num = df[num_cols].reset_index(drop='True')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "84162fd7-567c-404a-84c3-d0034fad6fc3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['mrd_type', 'postal_code', 'elementary_school_district',\n",
       "       'middle_or_junior_school_district', 'high_school_district',\n",
       "       'waterfront_yn'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_cat.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d1a1452b-88a6-4b8d-8457-fd500a0eb99d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['original_list_price', 'list_price', 'association_fee',\n",
       "       'tax_annual_amount', 'days_on_market', 'rooms_total', 'bedrooms_total',\n",
       "       'bathrooms_full', 'bathrooms_half', 'garage_spaces', 'lot_size_acres',\n",
       "       'living_area', 'year_built'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_num.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "56821749-7303-42e4-a886-99df4f7c5d7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 400040 entries, 0 to 400039\n",
      "Data columns (total 13 columns):\n",
      " #   Column               Non-Null Count   Dtype  \n",
      "---  ------               --------------   -----  \n",
      " 0   original_list_price  400040 non-null  float64\n",
      " 1   list_price           400040 non-null  float64\n",
      " 2   association_fee      400040 non-null  float64\n",
      " 3   tax_annual_amount    400040 non-null  float64\n",
      " 4   days_on_market       400040 non-null  int64  \n",
      " 5   rooms_total          400040 non-null  int64  \n",
      " 6   bedrooms_total       400040 non-null  int64  \n",
      " 7   bathrooms_full       400040 non-null  int64  \n",
      " 8   bathrooms_half       400040 non-null  int64  \n",
      " 9   garage_spaces        400040 non-null  float64\n",
      " 10  lot_size_acres       400040 non-null  float64\n",
      " 11  living_area          400040 non-null  int64  \n",
      " 12  year_built           400040 non-null  int64  \n",
      "dtypes: float64(6), int64(7)\n",
      "memory usage: 39.7 MB\n"
     ]
    }
   ],
   "source": [
    "df_num.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3adf111f-f66d-4a70-8961-37b8eca191ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 400040 entries, 0 to 400039\n",
      "Data columns (total 6 columns):\n",
      " #   Column                            Non-Null Count   Dtype \n",
      "---  ------                            --------------   ----- \n",
      " 0   mrd_type                          400040 non-null  string\n",
      " 1   postal_code                       400040 non-null  string\n",
      " 2   elementary_school_district        400040 non-null  string\n",
      " 3   middle_or_junior_school_district  400040 non-null  string\n",
      " 4   high_school_district              400040 non-null  string\n",
      " 5   waterfront_yn                     400040 non-null  string\n",
      "dtypes: string(6)\n",
      "memory usage: 18.3 MB\n"
     ]
    }
   ],
   "source": [
    "df_cat.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b6ec148e-2c74-46a9-b199-3894cdc4c217",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 400040 entries, 0 to 400039\n",
      "Data columns (total 1 columns):\n",
      " #   Column       Non-Null Count   Dtype  \n",
      "---  ------       --------------   -----  \n",
      " 0   close_price  400040 non-null  float64\n",
      "dtypes: float64(1)\n",
      "memory usage: 3.1 MB\n"
     ]
    }
   ],
   "source": [
    "df_target.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f21d305e-7a7f-40a4-a859-bbf11676d904",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mrd_type_Attached Single</th>\n",
       "      <th>mrd_type_Detached Single</th>\n",
       "      <th>postal_code_46075</th>\n",
       "      <th>postal_code_46168</th>\n",
       "      <th>postal_code_46324</th>\n",
       "      <th>postal_code_46360</th>\n",
       "      <th>postal_code_46373</th>\n",
       "      <th>postal_code_47804</th>\n",
       "      <th>postal_code_47847</th>\n",
       "      <th>postal_code_47862</th>\n",
       "      <th>...</th>\n",
       "      <th>high_school_district_VALM</th>\n",
       "      <th>high_school_district_WAS3</th>\n",
       "      <th>high_school_district_WATE</th>\n",
       "      <th>high_school_district_WEBS</th>\n",
       "      <th>high_school_district_WENT</th>\n",
       "      <th>high_school_district_keno</th>\n",
       "      <th>high_school_district_kenos</th>\n",
       "      <th>high_school_district_salem</th>\n",
       "      <th>waterfront_yn_False</th>\n",
       "      <th>waterfront_yn_True</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 2337 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   mrd_type_Attached Single  mrd_type_Detached Single  postal_code_46075  \\\n",
       "0                       1.0                       0.0                0.0   \n",
       "1                       1.0                       0.0                0.0   \n",
       "2                       1.0                       0.0                0.0   \n",
       "3                       1.0                       0.0                0.0   \n",
       "4                       1.0                       0.0                0.0   \n",
       "\n",
       "   postal_code_46168  postal_code_46324  postal_code_46360  postal_code_46373  \\\n",
       "0                0.0                0.0                0.0                0.0   \n",
       "1                0.0                0.0                0.0                0.0   \n",
       "2                0.0                0.0                0.0                0.0   \n",
       "3                0.0                0.0                0.0                0.0   \n",
       "4                0.0                0.0                0.0                0.0   \n",
       "\n",
       "   postal_code_47804  postal_code_47847  postal_code_47862  ...  \\\n",
       "0                0.0                0.0                0.0  ...   \n",
       "1                0.0                0.0                0.0  ...   \n",
       "2                0.0                0.0                0.0  ...   \n",
       "3                0.0                0.0                0.0  ...   \n",
       "4                0.0                0.0                0.0  ...   \n",
       "\n",
       "   high_school_district_VALM  high_school_district_WAS3  \\\n",
       "0                        0.0                        0.0   \n",
       "1                        0.0                        0.0   \n",
       "2                        0.0                        0.0   \n",
       "3                        0.0                        0.0   \n",
       "4                        0.0                        0.0   \n",
       "\n",
       "   high_school_district_WATE  high_school_district_WEBS  \\\n",
       "0                        0.0                        0.0   \n",
       "1                        0.0                        0.0   \n",
       "2                        0.0                        0.0   \n",
       "3                        0.0                        0.0   \n",
       "4                        0.0                        0.0   \n",
       "\n",
       "   high_school_district_WENT  high_school_district_keno  \\\n",
       "0                        0.0                        0.0   \n",
       "1                        0.0                        0.0   \n",
       "2                        0.0                        0.0   \n",
       "3                        0.0                        0.0   \n",
       "4                        0.0                        0.0   \n",
       "\n",
       "   high_school_district_kenos  high_school_district_salem  \\\n",
       "0                         0.0                         0.0   \n",
       "1                         0.0                         0.0   \n",
       "2                         0.0                         0.0   \n",
       "3                         0.0                         0.0   \n",
       "4                         0.0                         0.0   \n",
       "\n",
       "   waterfront_yn_False  waterfront_yn_True  \n",
       "0                  0.0                 1.0  \n",
       "1                  0.0                 1.0  \n",
       "2                  0.0                 1.0  \n",
       "3                  0.0                 1.0  \n",
       "4                  0.0                 1.0  \n",
       "\n",
       "[5 rows x 2337 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "ohe = OneHotEncoder(categories='auto')\n",
    "feature_arr = ohe.fit_transform(df_cat).toarray()\n",
    "feature_labels = ohe.get_feature_names_out()\n",
    "df_cat_encoded = pd.DataFrame(feature_arr, columns=feature_labels)\n",
    "df_cat_encoded.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2a27811f-518c-43a0-9daf-9ea1b84006ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_features = pd.concat([df_num,df_cat_encoded], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "93c20253-70f0-446a-803d-d8e77ec57beb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 400040 entries, 0 to 400039\n",
      "Columns: 2350 entries, original_list_price to waterfront_yn_True\n",
      "dtypes: float64(2343), int64(7)\n",
      "memory usage: 7.0 GB\n"
     ]
    }
   ],
   "source": [
    "df_features.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5f8cf5eb-3669-4fc2-ae38-d675a0cd3cbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 400040 entries, 0 to 400039\n",
      "Data columns (total 1 columns):\n",
      " #   Column          Non-Null Count   Dtype \n",
      "---  ------          --------------   ----- \n",
      " 0   public_remarks  400040 non-null  string\n",
      "dtypes: string(1)\n",
      "memory usage: 3.1 MB\n"
     ]
    }
   ],
   "source": [
    "df_text.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9e79dacd-1222-4d63-83f2-beeee2578819",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_features.isnull().values.any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "59b683fc-6ee1-4ee9-a378-d835a180dd22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False False\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_features, df_target, test_size=0.33)\n",
    "\n",
    "min_max_scale = MinMaxScaler()\n",
    "X_train[num_cols] = min_max_scale.fit_transform(X_train[num_cols])\n",
    "X_test[num_cols] = min_max_scale.transform(X_test[num_cols])\n",
    "\n",
    "print(X_train.isnull().values.any(),X_test.isnull().values.any())\n",
    "\n",
    "X_train = torch.from_numpy(X_train.values).type(torch.float).to(device)\n",
    "X_test = torch.from_numpy(X_test.values).type(torch.float).to(device)\n",
    "y_train = torch.from_numpy(y_train.values).type(torch.float).to(device)\n",
    "y_test = torch.from_numpy(y_test.values).type(torch.float).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6f0bf64c-ee2f-479a-afef-3e0b411dd415",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([268026, 2350]),\n",
       " torch.Size([132014, 2350]),\n",
       " torch.Size([268026, 1]),\n",
       " torch.Size([132014, 1]))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "32f6a40d-d2c0-464e-b981-b8fab5ab7bb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = list(zip(X_train,y_train))\n",
    "test_data = list(zip(X_test,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9a2db91b-ac81-4f88-8d15-c509840fc747",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = torch.utils.data.DataLoader(train_data, shuffle=False, batch_size=BATCH_SIZE)\n",
    "test_dataloader = torch.utils.data.DataLoader(test_data, shuffle=False, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "70d8371f-756b-4b8c-9257-a2bef68f5d33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([64, 2350]),\n",
       " torch.Size([64, 1]),\n",
       " device(type='cuda', index=0),\n",
       " device(type='cuda', index=0))"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp_x, tmp_y, = next(iter(train_dataloader))\n",
    "tmp_x.shape,tmp_y.shape, tmp_x.device, tmp_y.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1a1344e9-4fc0-4262-86c8-8ef11d52cb44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c259b798-f715-47a5-a349-065fd2e5db6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = tmp_x.shape[-1]\n",
    "output_shape = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "16461709-6032-4f8c-8564-b23565fc83c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RealEstatePredictor(nn.Module):\n",
    "    def __init__(self, input_shape: int, hidden_units_1: int, hidden_units_2: int, output_shape: int, p: float):\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(in_features=input_shape, out_features=hidden_units_1)\n",
    "        self.hidden_act1 = nn.ReLU()\n",
    "        self.dropout_layer1 = nn.Dropout(p=p)\n",
    "        self.linear2 = nn.Linear(in_features=hidden_units_1, out_features=hidden_units_2)\n",
    "        self.hidden_act2 = nn.ReLU()\n",
    "        self.dropout_layer2 = nn.Dropout(p=p)\n",
    "        self.linear3 = nn.Linear(in_features=hidden_units_2, out_features=output_shape)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.linear1(x)\n",
    "        x = self.hidden_act1(x)\n",
    "        x = self.dropout_layer1(x)\n",
    "        x = self.linear2(x)\n",
    "        x = self.hidden_act2(x)\n",
    "        x = self.dropout_layer2(x)\n",
    "        x = self.linear3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "0be73893-626b-45a2-b59a-008df5025b24",
   "metadata": {},
   "outputs": [],
   "source": [
    "def weights_init_uniform_rule(m):\n",
    "    classname = m.__class__.__name__\n",
    "    # for every Linear layer in a model..\n",
    "    if classname.find('Linear') != -1:\n",
    "        # get the number of the inputs\n",
    "        n = m.in_features\n",
    "        y = 1.0/np.sqrt(n)\n",
    "        m.weight.data.uniform_(-y, y)\n",
    "        m.bias.data.fill_(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "2e5b590e-ae0a-4034-85b9-743d9d17aa39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RealEstatePredictor(\n",
       "  (linear1): Linear(in_features=2350, out_features=100, bias=True)\n",
       "  (hidden_act1): ReLU()\n",
       "  (dropout_layer1): Dropout(p=0.2, inplace=False)\n",
       "  (linear2): Linear(in_features=100, out_features=100, bias=True)\n",
       "  (hidden_act2): ReLU()\n",
       "  (dropout_layer2): Dropout(p=0.2, inplace=False)\n",
       "  (linear3): Linear(in_features=100, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_0 = RealEstatePredictor(input_shape=input_shape,\n",
    "    hidden_units_1=100, # how many units in the hiden layer\n",
    "    hidden_units_2=100, # how many units in the hiden layer\n",
    "    output_shape=output_shape, # one for every class\n",
    "    p = .2\n",
    ")\n",
    "model_0.apply(weights_init_uniform_rule)\n",
    "model_0.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "b7b71031-cd72-4b92-aa93-6d8fc727c341",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('linear1.weight',\n",
       "              tensor([[ 0.0020,  0.0074, -0.0044,  ..., -0.0202, -0.0136,  0.0175],\n",
       "                      [ 0.0066,  0.0105, -0.0150,  ..., -0.0015,  0.0151,  0.0200],\n",
       "                      [-0.0022,  0.0138,  0.0082,  ...,  0.0024,  0.0189,  0.0074],\n",
       "                      ...,\n",
       "                      [ 0.0146, -0.0158, -0.0171,  ...,  0.0018,  0.0137, -0.0173],\n",
       "                      [-0.0075, -0.0024,  0.0128,  ..., -0.0049, -0.0111,  0.0030],\n",
       "                      [ 0.0002, -0.0083,  0.0072,  ..., -0.0135,  0.0195, -0.0066]],\n",
       "                     device='cuda:0')),\n",
       "             ('linear1.bias',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0.], device='cuda:0')),\n",
       "             ('linear2.weight',\n",
       "              tensor([[ 0.0049, -0.0297, -0.0703,  ..., -0.0707, -0.0285, -0.0410],\n",
       "                      [-0.0481, -0.0001, -0.0893,  ..., -0.0505,  0.0365, -0.0814],\n",
       "                      [-0.0878,  0.0254,  0.0347,  ..., -0.0831, -0.0763, -0.0302],\n",
       "                      ...,\n",
       "                      [-0.0748, -0.0516, -0.0054,  ...,  0.0245, -0.0615, -0.0091],\n",
       "                      [ 0.0952, -0.0884,  0.0281,  ..., -0.0670, -0.0183,  0.0586],\n",
       "                      [ 0.0142,  0.0083, -0.0709,  ...,  0.0053,  0.0643,  0.0068]],\n",
       "                     device='cuda:0')),\n",
       "             ('linear2.bias',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0.], device='cuda:0')),\n",
       "             ('linear3.weight',\n",
       "              tensor([[-0.0345,  0.0129, -0.0539, -0.0445, -0.0489,  0.0708,  0.0388, -0.0844,\n",
       "                       -0.0553,  0.0110,  0.0628, -0.0186,  0.0859,  0.0463, -0.0311, -0.0066,\n",
       "                       -0.0121, -0.0909,  0.0920,  0.0598, -0.0617,  0.0848, -0.0748, -0.0398,\n",
       "                       -0.0347,  0.0459,  0.0201, -0.0197, -0.0761, -0.0940, -0.0809, -0.0733,\n",
       "                       -0.0559, -0.0842, -0.0639, -0.0148, -0.0235, -0.0515, -0.0786, -0.0326,\n",
       "                        0.0976, -0.0742, -0.0391,  0.0879,  0.0433, -0.0022, -0.0637, -0.0820,\n",
       "                       -0.0941, -0.0990, -0.0245, -0.0488,  0.0103,  0.0147, -0.0437,  0.0266,\n",
       "                       -0.0679,  0.0874,  0.0004, -0.0256, -0.0319,  0.0308,  0.0321,  0.0349,\n",
       "                       -0.0695,  0.0720, -0.0610,  0.0979, -0.0369, -0.0188,  0.0990,  0.0638,\n",
       "                       -0.0422,  0.0927, -0.0152,  0.0663,  0.0899, -0.0775, -0.0739,  0.0243,\n",
       "                        0.0825,  0.0160,  0.0707,  0.0184, -0.0928, -0.0552, -0.0238, -0.0321,\n",
       "                       -0.0608,  0.0738, -0.0605,  0.0288, -0.0052,  0.0592, -0.0692,  0.0345,\n",
       "                       -0.0375, -0.0230, -0.0634, -0.0179]], device='cuda:0')),\n",
       "             ('linear3.bias', tensor([0.], device='cuda:0'))])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_0.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "712b13a3-807b-47cc-8ee5-647a27eb1d15",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.MSELoss()\n",
    "# optimizer = torch.optim.SGD(params=model_0.parameters(), lr=0.01)\n",
    "optimizer = torch.optim.Adam(params=model_0.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "b27f032d-e363-4876-9fc6-eaf9adacbad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from timeit import default_timer as timer \n",
    "def print_train_time(start: float, end: float, device: torch.device = None):\n",
    "    \"\"\"Prints difference between start and end time.\n",
    "\n",
    "    Args:\n",
    "        start (float): Start time of computation (preferred in timeit format). \n",
    "        end (float): End time of computation.\n",
    "        device ([type], optional): Device that compute is running on. Defaults to None.\n",
    "\n",
    "    Returns:\n",
    "        float: time between start and end in seconds (higher is longer).\n",
    "    \"\"\"\n",
    "    total_time = end - start\n",
    "    print(f\"Train time on {device}: {total_time:.3f} seconds\")\n",
    "    return total_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "3b488c7e-ce2b-4862-af98-2e87f2fe4669",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76dca85dbf17486693deeaf539567ec2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/125 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n",
      "-------\n",
      "Looked at 0/268026 samples\n",
      "Looked at 128000/268026 samples\n",
      "Looked at 256000/268026 samples\n",
      "\n",
      "Train loss: 48752566272.00000 | Test loss: 15855105024.00000\n",
      "Epoch: 1\n",
      "-------\n",
      "Looked at 0/268026 samples\n",
      "Looked at 128000/268026 samples\n",
      "Looked at 256000/268026 samples\n",
      "\n",
      "Train loss: 12018121728.00000 | Test loss: 6196777472.00000\n",
      "Epoch: 2\n",
      "-------\n",
      "Looked at 0/268026 samples\n",
      "Looked at 128000/268026 samples\n",
      "Looked at 256000/268026 samples\n",
      "\n",
      "Train loss: 6516444160.00000 | Test loss: 4320909312.00000\n",
      "Epoch: 3\n",
      "-------\n",
      "Looked at 0/268026 samples\n",
      "Looked at 128000/268026 samples\n",
      "Looked at 256000/268026 samples\n",
      "\n",
      "Train loss: 5179950080.00000 | Test loss: 3689318400.00000\n",
      "Epoch: 4\n",
      "-------\n",
      "Looked at 0/268026 samples\n",
      "Looked at 128000/268026 samples\n",
      "Looked at 256000/268026 samples\n",
      "\n",
      "Train loss: 4684831744.00000 | Test loss: 3237732352.00000\n",
      "Epoch: 5\n",
      "-------\n",
      "Looked at 0/268026 samples\n",
      "Looked at 128000/268026 samples\n",
      "Looked at 256000/268026 samples\n",
      "\n",
      "Train loss: 4097004544.00000 | Test loss: 3080913152.00000\n",
      "Epoch: 6\n",
      "-------\n",
      "Looked at 0/268026 samples\n",
      "Looked at 128000/268026 samples\n",
      "Looked at 256000/268026 samples\n",
      "\n",
      "Train loss: 4096228608.00000 | Test loss: 3031662336.00000\n",
      "Epoch: 7\n",
      "-------\n",
      "Looked at 0/268026 samples\n",
      "Looked at 128000/268026 samples\n",
      "Looked at 256000/268026 samples\n",
      "\n",
      "Train loss: 4142735360.00000 | Test loss: 2822878720.00000\n",
      "Epoch: 8\n",
      "-------\n",
      "Looked at 0/268026 samples\n",
      "Looked at 128000/268026 samples\n",
      "Looked at 256000/268026 samples\n",
      "\n",
      "Train loss: 3951151872.00000 | Test loss: 2827190784.00000\n",
      "Epoch: 9\n",
      "-------\n",
      "Looked at 0/268026 samples\n",
      "Looked at 128000/268026 samples\n",
      "Looked at 256000/268026 samples\n",
      "\n",
      "Train loss: 3886483968.00000 | Test loss: 2795348224.00000\n",
      "Epoch: 10\n",
      "-------\n",
      "Looked at 0/268026 samples\n",
      "Looked at 128000/268026 samples\n",
      "Looked at 256000/268026 samples\n",
      "\n",
      "Train loss: 3813786368.00000 | Test loss: 2878865664.00000\n",
      "Epoch: 11\n",
      "-------\n",
      "Looked at 0/268026 samples\n",
      "Looked at 128000/268026 samples\n",
      "Looked at 256000/268026 samples\n",
      "\n",
      "Train loss: 3998031872.00000 | Test loss: 2677989120.00000\n",
      "Epoch: 12\n",
      "-------\n",
      "Looked at 0/268026 samples\n",
      "Looked at 128000/268026 samples\n",
      "Looked at 256000/268026 samples\n",
      "\n",
      "Train loss: 3682969856.00000 | Test loss: 2757887232.00000\n",
      "Epoch: 13\n",
      "-------\n",
      "Looked at 0/268026 samples\n",
      "Looked at 128000/268026 samples\n",
      "Looked at 256000/268026 samples\n",
      "\n",
      "Train loss: 3763460096.00000 | Test loss: 2588222464.00000\n",
      "Epoch: 14\n",
      "-------\n",
      "Looked at 0/268026 samples\n",
      "Looked at 128000/268026 samples\n",
      "Looked at 256000/268026 samples\n",
      "\n",
      "Train loss: 3669263872.00000 | Test loss: 2526049792.00000\n",
      "Epoch: 15\n",
      "-------\n",
      "Looked at 0/268026 samples\n",
      "Looked at 128000/268026 samples\n",
      "Looked at 256000/268026 samples\n",
      "\n",
      "Train loss: 3741490432.00000 | Test loss: 2644227584.00000\n",
      "Epoch: 16\n",
      "-------\n",
      "Looked at 0/268026 samples\n",
      "Looked at 128000/268026 samples\n",
      "Looked at 256000/268026 samples\n",
      "\n",
      "Train loss: 3651764992.00000 | Test loss: 2675838720.00000\n",
      "Epoch: 17\n",
      "-------\n",
      "Looked at 0/268026 samples\n",
      "Looked at 128000/268026 samples\n",
      "Looked at 256000/268026 samples\n",
      "\n",
      "Train loss: 3481039872.00000 | Test loss: 2636526848.00000\n",
      "Epoch: 18\n",
      "-------\n",
      "Looked at 0/268026 samples\n",
      "Looked at 128000/268026 samples\n",
      "Looked at 256000/268026 samples\n",
      "\n",
      "Train loss: 3831020032.00000 | Test loss: 2493349888.00000\n",
      "Epoch: 19\n",
      "-------\n",
      "Looked at 0/268026 samples\n",
      "Looked at 128000/268026 samples\n",
      "Looked at 256000/268026 samples\n",
      "\n",
      "Train loss: 3746179328.00000 | Test loss: 2570688512.00000\n",
      "Epoch: 20\n",
      "-------\n",
      "Looked at 0/268026 samples\n",
      "Looked at 128000/268026 samples\n",
      "Looked at 256000/268026 samples\n",
      "\n",
      "Train loss: 3495806208.00000 | Test loss: 2633907456.00000\n",
      "Epoch: 21\n",
      "-------\n",
      "Looked at 0/268026 samples\n",
      "Looked at 128000/268026 samples\n",
      "Looked at 256000/268026 samples\n",
      "\n",
      "Train loss: 3535107584.00000 | Test loss: 2745563648.00000\n",
      "Epoch: 22\n",
      "-------\n",
      "Looked at 0/268026 samples\n",
      "Looked at 128000/268026 samples\n",
      "Looked at 256000/268026 samples\n",
      "\n",
      "Train loss: 3518803200.00000 | Test loss: 2537811968.00000\n",
      "Epoch: 23\n",
      "-------\n",
      "Looked at 0/268026 samples\n",
      "Looked at 128000/268026 samples\n",
      "Looked at 256000/268026 samples\n",
      "\n",
      "Train loss: 3753895936.00000 | Test loss: 2601176320.00000\n",
      "Epoch: 24\n",
      "-------\n",
      "Looked at 0/268026 samples\n",
      "Looked at 128000/268026 samples\n",
      "Looked at 256000/268026 samples\n",
      "\n",
      "Train loss: 3425085952.00000 | Test loss: 2616011776.00000\n",
      "Epoch: 25\n",
      "-------\n",
      "Looked at 0/268026 samples\n",
      "Looked at 128000/268026 samples\n",
      "Looked at 256000/268026 samples\n",
      "\n",
      "Train loss: 3467312896.00000 | Test loss: 2625044480.00000\n",
      "Epoch: 26\n",
      "-------\n",
      "Looked at 0/268026 samples\n",
      "Looked at 128000/268026 samples\n",
      "Looked at 256000/268026 samples\n",
      "\n",
      "Train loss: 3256996352.00000 | Test loss: 2525064960.00000\n",
      "Epoch: 27\n",
      "-------\n",
      "Looked at 0/268026 samples\n",
      "Looked at 128000/268026 samples\n",
      "Looked at 256000/268026 samples\n",
      "\n",
      "Train loss: 3489410304.00000 | Test loss: 2645309184.00000\n",
      "Epoch: 28\n",
      "-------\n",
      "Looked at 0/268026 samples\n",
      "Looked at 128000/268026 samples\n",
      "Looked at 256000/268026 samples\n",
      "\n",
      "Train loss: 3493233152.00000 | Test loss: 2461428480.00000\n",
      "Epoch: 29\n",
      "-------\n",
      "Looked at 0/268026 samples\n",
      "Looked at 128000/268026 samples\n",
      "Looked at 256000/268026 samples\n",
      "\n",
      "Train loss: 3425230336.00000 | Test loss: 2475232512.00000\n",
      "Epoch: 30\n",
      "-------\n",
      "Looked at 0/268026 samples\n",
      "Looked at 128000/268026 samples\n",
      "Looked at 256000/268026 samples\n",
      "\n",
      "Train loss: 3399868672.00000 | Test loss: 2593954304.00000\n",
      "Epoch: 31\n",
      "-------\n",
      "Looked at 0/268026 samples\n",
      "Looked at 128000/268026 samples\n",
      "Looked at 256000/268026 samples\n",
      "\n",
      "Train loss: 3425945088.00000 | Test loss: 2642615808.00000\n",
      "Epoch: 32\n",
      "-------\n",
      "Looked at 0/268026 samples\n",
      "Looked at 128000/268026 samples\n",
      "Looked at 256000/268026 samples\n",
      "\n",
      "Train loss: 3243227392.00000 | Test loss: 2720904960.00000\n",
      "Epoch: 33\n",
      "-------\n",
      "Looked at 0/268026 samples\n",
      "Looked at 128000/268026 samples\n",
      "Looked at 256000/268026 samples\n",
      "\n",
      "Train loss: 3246677248.00000 | Test loss: 2536272896.00000\n",
      "Epoch: 34\n",
      "-------\n",
      "Looked at 0/268026 samples\n",
      "Looked at 128000/268026 samples\n",
      "Looked at 256000/268026 samples\n",
      "\n",
      "Train loss: 3252622592.00000 | Test loss: 2681037568.00000\n",
      "Epoch: 35\n",
      "-------\n",
      "Looked at 0/268026 samples\n",
      "Looked at 128000/268026 samples\n",
      "Looked at 256000/268026 samples\n",
      "\n",
      "Train loss: 3277197824.00000 | Test loss: 2769789184.00000\n",
      "Epoch: 36\n",
      "-------\n",
      "Looked at 0/268026 samples\n",
      "Looked at 128000/268026 samples\n",
      "Looked at 256000/268026 samples\n",
      "\n",
      "Train loss: 3318952192.00000 | Test loss: 2510778880.00000\n",
      "Epoch: 37\n",
      "-------\n",
      "Looked at 0/268026 samples\n",
      "Looked at 128000/268026 samples\n",
      "Looked at 256000/268026 samples\n",
      "\n",
      "Train loss: 3240653312.00000 | Test loss: 2734480896.00000\n",
      "Epoch: 38\n",
      "-------\n",
      "Looked at 0/268026 samples\n",
      "Looked at 128000/268026 samples\n",
      "Looked at 256000/268026 samples\n",
      "\n",
      "Train loss: 3262290688.00000 | Test loss: 2528067584.00000\n",
      "Epoch: 39\n",
      "-------\n",
      "Looked at 0/268026 samples\n",
      "Looked at 128000/268026 samples\n",
      "Looked at 256000/268026 samples\n",
      "\n",
      "Train loss: 3454564864.00000 | Test loss: 2663294208.00000\n",
      "Epoch: 40\n",
      "-------\n",
      "Looked at 0/268026 samples\n",
      "Looked at 128000/268026 samples\n",
      "Looked at 256000/268026 samples\n",
      "\n",
      "Train loss: 3376593920.00000 | Test loss: 2596773632.00000\n",
      "Epoch: 41\n",
      "-------\n",
      "Looked at 0/268026 samples\n",
      "Looked at 128000/268026 samples\n",
      "Looked at 256000/268026 samples\n",
      "\n",
      "Train loss: 3201207296.00000 | Test loss: 2534835200.00000\n",
      "Epoch: 42\n",
      "-------\n",
      "Looked at 0/268026 samples\n",
      "Looked at 128000/268026 samples\n",
      "Looked at 256000/268026 samples\n",
      "\n",
      "Train loss: 3152864512.00000 | Test loss: 2492252928.00000\n",
      "Epoch: 43\n",
      "-------\n",
      "Looked at 0/268026 samples\n",
      "Looked at 128000/268026 samples\n",
      "Looked at 256000/268026 samples\n",
      "\n",
      "Train loss: 3265661952.00000 | Test loss: 2450150400.00000\n",
      "Epoch: 44\n",
      "-------\n",
      "Looked at 0/268026 samples\n",
      "Looked at 128000/268026 samples\n",
      "Looked at 256000/268026 samples\n",
      "\n",
      "Train loss: 3327487744.00000 | Test loss: 2461887744.00000\n",
      "Epoch: 45\n",
      "-------\n",
      "Looked at 0/268026 samples\n",
      "Looked at 128000/268026 samples\n",
      "Looked at 256000/268026 samples\n",
      "\n",
      "Train loss: 3164239872.00000 | Test loss: 2520783360.00000\n",
      "Epoch: 46\n",
      "-------\n",
      "Looked at 0/268026 samples\n",
      "Looked at 128000/268026 samples\n",
      "Looked at 256000/268026 samples\n",
      "\n",
      "Train loss: 3082008064.00000 | Test loss: 2423554304.00000\n",
      "Epoch: 47\n",
      "-------\n",
      "Looked at 0/268026 samples\n",
      "Looked at 128000/268026 samples\n",
      "Looked at 256000/268026 samples\n",
      "\n",
      "Train loss: 3323437824.00000 | Test loss: 2385801728.00000\n",
      "Epoch: 48\n",
      "-------\n",
      "Looked at 0/268026 samples\n",
      "Looked at 128000/268026 samples\n",
      "Looked at 256000/268026 samples\n",
      "\n",
      "Train loss: 3107711488.00000 | Test loss: 2480883456.00000\n",
      "Epoch: 49\n",
      "-------\n",
      "Looked at 0/268026 samples\n",
      "Looked at 128000/268026 samples\n",
      "Looked at 256000/268026 samples\n",
      "\n",
      "Train loss: 3107113216.00000 | Test loss: 2567157760.00000\n",
      "Epoch: 50\n",
      "-------\n",
      "Looked at 0/268026 samples\n",
      "Looked at 128000/268026 samples\n",
      "Looked at 256000/268026 samples\n",
      "\n",
      "Train loss: 3135690496.00000 | Test loss: 2506983680.00000\n",
      "Epoch: 51\n",
      "-------\n",
      "Looked at 0/268026 samples\n",
      "Looked at 128000/268026 samples\n",
      "Looked at 256000/268026 samples\n",
      "\n",
      "Train loss: 3131101440.00000 | Test loss: 2499266048.00000\n",
      "Epoch: 52\n",
      "-------\n",
      "Looked at 0/268026 samples\n",
      "Looked at 128000/268026 samples\n",
      "Looked at 256000/268026 samples\n",
      "\n",
      "Train loss: 3045156864.00000 | Test loss: 2443095040.00000\n",
      "Epoch: 53\n",
      "-------\n",
      "Looked at 0/268026 samples\n",
      "Looked at 128000/268026 samples\n",
      "Looked at 256000/268026 samples\n",
      "\n",
      "Train loss: 3135353344.00000 | Test loss: 2592787968.00000\n",
      "Epoch: 54\n",
      "-------\n",
      "Looked at 0/268026 samples\n",
      "Looked at 128000/268026 samples\n",
      "Looked at 256000/268026 samples\n",
      "\n",
      "Train loss: 3131540992.00000 | Test loss: 2601066752.00000\n",
      "Epoch: 55\n",
      "-------\n",
      "Looked at 0/268026 samples\n",
      "Looked at 128000/268026 samples\n",
      "Looked at 256000/268026 samples\n",
      "\n",
      "Train loss: 2987824896.00000 | Test loss: 2721664512.00000\n",
      "Epoch: 56\n",
      "-------\n",
      "Looked at 0/268026 samples\n",
      "Looked at 128000/268026 samples\n",
      "Looked at 256000/268026 samples\n",
      "\n",
      "Train loss: 3046883328.00000 | Test loss: 2545973248.00000\n",
      "Epoch: 57\n",
      "-------\n",
      "Looked at 0/268026 samples\n",
      "Looked at 128000/268026 samples\n",
      "Looked at 256000/268026 samples\n",
      "\n",
      "Train loss: 3048155392.00000 | Test loss: 2616748032.00000\n",
      "Epoch: 58\n",
      "-------\n",
      "Looked at 0/268026 samples\n",
      "Looked at 128000/268026 samples\n",
      "Looked at 256000/268026 samples\n",
      "\n",
      "Train loss: 2967736576.00000 | Test loss: 2410753536.00000\n",
      "Epoch: 59\n",
      "-------\n",
      "Looked at 0/268026 samples\n",
      "Looked at 128000/268026 samples\n",
      "Looked at 256000/268026 samples\n",
      "\n",
      "Train loss: 3180792576.00000 | Test loss: 2641590528.00000\n",
      "Epoch: 60\n",
      "-------\n",
      "Looked at 0/268026 samples\n",
      "Looked at 128000/268026 samples\n",
      "Looked at 256000/268026 samples\n",
      "\n",
      "Train loss: 3111834624.00000 | Test loss: 2861748224.00000\n",
      "Epoch: 61\n",
      "-------\n",
      "Looked at 0/268026 samples\n",
      "Looked at 128000/268026 samples\n",
      "Looked at 256000/268026 samples\n",
      "\n",
      "Train loss: 3122271232.00000 | Test loss: 2804726784.00000\n",
      "Epoch: 62\n",
      "-------\n",
      "Looked at 0/268026 samples\n",
      "Looked at 128000/268026 samples\n",
      "Looked at 256000/268026 samples\n",
      "\n",
      "Train loss: 2985751808.00000 | Test loss: 2540284672.00000\n",
      "Epoch: 63\n",
      "-------\n",
      "Looked at 0/268026 samples\n",
      "Looked at 128000/268026 samples\n",
      "Looked at 256000/268026 samples\n",
      "\n",
      "Train loss: 2977519872.00000 | Test loss: 2518176768.00000\n",
      "Epoch: 64\n",
      "-------\n",
      "Looked at 0/268026 samples\n",
      "Looked at 128000/268026 samples\n",
      "Looked at 256000/268026 samples\n",
      "\n",
      "Train loss: 2933614336.00000 | Test loss: 2394138112.00000\n",
      "Epoch: 65\n",
      "-------\n",
      "Looked at 0/268026 samples\n",
      "Looked at 128000/268026 samples\n",
      "Looked at 256000/268026 samples\n",
      "\n",
      "Train loss: 3021628928.00000 | Test loss: 2440212224.00000\n",
      "Epoch: 66\n",
      "-------\n",
      "Looked at 0/268026 samples\n",
      "Looked at 128000/268026 samples\n",
      "Looked at 256000/268026 samples\n",
      "\n",
      "Train loss: 2976902912.00000 | Test loss: 2479783680.00000\n",
      "Epoch: 67\n",
      "-------\n",
      "Looked at 0/268026 samples\n",
      "Looked at 128000/268026 samples\n",
      "Looked at 256000/268026 samples\n",
      "\n",
      "Train loss: 2919810560.00000 | Test loss: 2463627264.00000\n",
      "Epoch: 68\n",
      "-------\n",
      "Looked at 0/268026 samples\n",
      "Looked at 128000/268026 samples\n",
      "Looked at 256000/268026 samples\n",
      "\n",
      "Train loss: 2986334976.00000 | Test loss: 2517821440.00000\n",
      "Epoch: 69\n",
      "-------\n",
      "Looked at 0/268026 samples\n",
      "Looked at 128000/268026 samples\n",
      "Looked at 256000/268026 samples\n",
      "\n",
      "Train loss: 2957165312.00000 | Test loss: 2594947072.00000\n",
      "Epoch: 70\n",
      "-------\n",
      "Looked at 0/268026 samples\n",
      "Looked at 128000/268026 samples\n",
      "Looked at 256000/268026 samples\n",
      "\n",
      "Train loss: 3043453184.00000 | Test loss: 2558147072.00000\n",
      "Epoch: 71\n",
      "-------\n",
      "Looked at 0/268026 samples\n",
      "Looked at 128000/268026 samples\n",
      "Looked at 256000/268026 samples\n",
      "\n",
      "Train loss: 3129277952.00000 | Test loss: 2809575168.00000\n",
      "Epoch: 72\n",
      "-------\n",
      "Looked at 0/268026 samples\n",
      "Looked at 128000/268026 samples\n",
      "Looked at 256000/268026 samples\n",
      "\n",
      "Train loss: 2888266240.00000 | Test loss: 2588780800.00000\n",
      "Epoch: 73\n",
      "-------\n",
      "Looked at 0/268026 samples\n",
      "Looked at 128000/268026 samples\n",
      "Looked at 256000/268026 samples\n",
      "\n",
      "Train loss: 2813375744.00000 | Test loss: 2426288128.00000\n",
      "Epoch: 74\n",
      "-------\n",
      "Looked at 0/268026 samples\n",
      "Looked at 128000/268026 samples\n",
      "Looked at 256000/268026 samples\n",
      "\n",
      "Train loss: 2976102912.00000 | Test loss: 2636581888.00000\n",
      "Epoch: 75\n",
      "-------\n",
      "Looked at 0/268026 samples\n",
      "Looked at 128000/268026 samples\n",
      "Looked at 256000/268026 samples\n",
      "\n",
      "Train loss: 3173979136.00000 | Test loss: 2465878272.00000\n",
      "Epoch: 76\n",
      "-------\n",
      "Looked at 0/268026 samples\n",
      "Looked at 128000/268026 samples\n",
      "Looked at 256000/268026 samples\n",
      "\n",
      "Train loss: 3127248384.00000 | Test loss: 2395927040.00000\n",
      "Epoch: 77\n",
      "-------\n",
      "Looked at 0/268026 samples\n",
      "Looked at 128000/268026 samples\n",
      "Looked at 256000/268026 samples\n",
      "\n",
      "Train loss: 2952534528.00000 | Test loss: 2677206016.00000\n",
      "Epoch: 78\n",
      "-------\n",
      "Looked at 0/268026 samples\n",
      "Looked at 128000/268026 samples\n",
      "Looked at 256000/268026 samples\n",
      "\n",
      "Train loss: 3062304512.00000 | Test loss: 2440956416.00000\n",
      "Epoch: 79\n",
      "-------\n",
      "Looked at 0/268026 samples\n",
      "Looked at 128000/268026 samples\n",
      "Looked at 256000/268026 samples\n",
      "\n",
      "Train loss: 3299658240.00000 | Test loss: 2580512000.00000\n",
      "Epoch: 80\n",
      "-------\n",
      "Looked at 0/268026 samples\n",
      "Looked at 128000/268026 samples\n",
      "Looked at 256000/268026 samples\n",
      "\n",
      "Train loss: 3026174720.00000 | Test loss: 2584128768.00000\n",
      "Epoch: 81\n",
      "-------\n",
      "Looked at 0/268026 samples\n",
      "Looked at 128000/268026 samples\n",
      "Looked at 256000/268026 samples\n",
      "\n",
      "Train loss: 3024089856.00000 | Test loss: 2533580032.00000\n",
      "Epoch: 82\n",
      "-------\n",
      "Looked at 0/268026 samples\n",
      "Looked at 128000/268026 samples\n",
      "Looked at 256000/268026 samples\n",
      "\n",
      "Train loss: 3020911872.00000 | Test loss: 2551135744.00000\n",
      "Epoch: 83\n",
      "-------\n",
      "Looked at 0/268026 samples\n",
      "Looked at 128000/268026 samples\n",
      "Looked at 256000/268026 samples\n",
      "\n",
      "Train loss: 2921312000.00000 | Test loss: 2460701696.00000\n",
      "Epoch: 84\n",
      "-------\n",
      "Looked at 0/268026 samples\n",
      "Looked at 128000/268026 samples\n",
      "Looked at 256000/268026 samples\n",
      "\n",
      "Train loss: 3022485248.00000 | Test loss: 2666982400.00000\n",
      "Epoch: 85\n",
      "-------\n",
      "Looked at 0/268026 samples\n",
      "Looked at 128000/268026 samples\n",
      "Looked at 256000/268026 samples\n",
      "\n",
      "Train loss: 3033646336.00000 | Test loss: 2486341376.00000\n",
      "Epoch: 86\n",
      "-------\n",
      "Looked at 0/268026 samples\n",
      "Looked at 128000/268026 samples\n",
      "Looked at 256000/268026 samples\n",
      "\n",
      "Train loss: 3022077184.00000 | Test loss: 2837786368.00000\n",
      "Epoch: 87\n",
      "-------\n",
      "Looked at 0/268026 samples\n",
      "Looked at 128000/268026 samples\n",
      "Looked at 256000/268026 samples\n",
      "\n",
      "Train loss: 2861594880.00000 | Test loss: 2667146496.00000\n",
      "Epoch: 88\n",
      "-------\n",
      "Looked at 0/268026 samples\n",
      "Looked at 128000/268026 samples\n",
      "Looked at 256000/268026 samples\n",
      "\n",
      "Train loss: 2866150912.00000 | Test loss: 2892722688.00000\n",
      "Epoch: 89\n",
      "-------\n",
      "Looked at 0/268026 samples\n",
      "Looked at 128000/268026 samples\n",
      "Looked at 256000/268026 samples\n",
      "\n",
      "Train loss: 2929776640.00000 | Test loss: 2687337216.00000\n",
      "Epoch: 90\n",
      "-------\n",
      "Looked at 0/268026 samples\n",
      "Looked at 128000/268026 samples\n",
      "Looked at 256000/268026 samples\n",
      "\n",
      "Train loss: 3133734400.00000 | Test loss: 2572328192.00000\n",
      "Epoch: 91\n",
      "-------\n",
      "Looked at 0/268026 samples\n",
      "Looked at 128000/268026 samples\n",
      "Looked at 256000/268026 samples\n",
      "\n",
      "Train loss: 2840830208.00000 | Test loss: 2446497280.00000\n",
      "Epoch: 92\n",
      "-------\n",
      "Looked at 0/268026 samples\n",
      "Looked at 128000/268026 samples\n",
      "Looked at 256000/268026 samples\n",
      "\n",
      "Train loss: 3266989568.00000 | Test loss: 2491306496.00000\n",
      "Epoch: 93\n",
      "-------\n",
      "Looked at 0/268026 samples\n",
      "Looked at 128000/268026 samples\n",
      "Looked at 256000/268026 samples\n",
      "\n",
      "Train loss: 2960943104.00000 | Test loss: 2566962176.00000\n",
      "Epoch: 94\n",
      "-------\n",
      "Looked at 0/268026 samples\n",
      "Looked at 128000/268026 samples\n",
      "Looked at 256000/268026 samples\n",
      "\n",
      "Train loss: 2998379008.00000 | Test loss: 2723269632.00000\n",
      "Epoch: 95\n",
      "-------\n",
      "Looked at 0/268026 samples\n",
      "Looked at 128000/268026 samples\n",
      "Looked at 256000/268026 samples\n",
      "\n",
      "Train loss: 2894857984.00000 | Test loss: 2622741760.00000\n",
      "Epoch: 96\n",
      "-------\n",
      "Looked at 0/268026 samples\n",
      "Looked at 128000/268026 samples\n",
      "Looked at 256000/268026 samples\n",
      "\n",
      "Train loss: 3066618624.00000 | Test loss: 2641009664.00000\n",
      "Epoch: 97\n",
      "-------\n",
      "Looked at 0/268026 samples\n",
      "Looked at 128000/268026 samples\n",
      "Looked at 256000/268026 samples\n",
      "\n",
      "Train loss: 2899799040.00000 | Test loss: 2700201216.00000\n",
      "Epoch: 98\n",
      "-------\n",
      "Looked at 0/268026 samples\n",
      "Looked at 128000/268026 samples\n",
      "Looked at 256000/268026 samples\n",
      "\n",
      "Train loss: 3014577152.00000 | Test loss: 2612660992.00000\n",
      "Epoch: 99\n",
      "-------\n",
      "Looked at 0/268026 samples\n",
      "Looked at 128000/268026 samples\n",
      "Looked at 256000/268026 samples\n",
      "\n",
      "Train loss: 2821708544.00000 | Test loss: 2447576064.00000\n",
      "Epoch: 100\n",
      "-------\n",
      "Looked at 0/268026 samples\n",
      "Looked at 128000/268026 samples\n",
      "Looked at 256000/268026 samples\n",
      "\n",
      "Train loss: 2926811648.00000 | Test loss: 2480957952.00000\n",
      "Epoch: 101\n",
      "-------\n",
      "Looked at 0/268026 samples\n",
      "Looked at 128000/268026 samples\n",
      "Looked at 256000/268026 samples\n",
      "\n",
      "Train loss: 2867824896.00000 | Test loss: 2662427648.00000\n",
      "Epoch: 102\n",
      "-------\n",
      "Looked at 0/268026 samples\n",
      "Looked at 128000/268026 samples\n",
      "Looked at 256000/268026 samples\n",
      "\n",
      "Train loss: 2929839104.00000 | Test loss: 2554111232.00000\n",
      "Epoch: 103\n",
      "-------\n",
      "Looked at 0/268026 samples\n",
      "Looked at 128000/268026 samples\n",
      "Looked at 256000/268026 samples\n",
      "\n",
      "Train loss: 2936392704.00000 | Test loss: 2606575104.00000\n",
      "Epoch: 104\n",
      "-------\n",
      "Looked at 0/268026 samples\n",
      "Looked at 128000/268026 samples\n",
      "Looked at 256000/268026 samples\n",
      "\n",
      "Train loss: 2852482816.00000 | Test loss: 2531122432.00000\n",
      "Epoch: 105\n",
      "-------\n",
      "Looked at 0/268026 samples\n",
      "Looked at 128000/268026 samples\n",
      "Looked at 256000/268026 samples\n",
      "\n",
      "Train loss: 2931708672.00000 | Test loss: 2476153856.00000\n",
      "Epoch: 106\n",
      "-------\n",
      "Looked at 0/268026 samples\n",
      "Looked at 128000/268026 samples\n",
      "Looked at 256000/268026 samples\n",
      "\n",
      "Train loss: 2940714496.00000 | Test loss: 2677900800.00000\n",
      "Epoch: 107\n",
      "-------\n",
      "Looked at 0/268026 samples\n",
      "Looked at 128000/268026 samples\n",
      "Looked at 256000/268026 samples\n",
      "\n",
      "Train loss: 2822371584.00000 | Test loss: 2553005824.00000\n",
      "Epoch: 108\n",
      "-------\n",
      "Looked at 0/268026 samples\n",
      "Looked at 128000/268026 samples\n",
      "Looked at 256000/268026 samples\n",
      "\n",
      "Train loss: 2959951104.00000 | Test loss: 2489404416.00000\n",
      "Epoch: 109\n",
      "-------\n",
      "Looked at 0/268026 samples\n",
      "Looked at 128000/268026 samples\n",
      "Looked at 256000/268026 samples\n",
      "\n",
      "Train loss: 2922560256.00000 | Test loss: 2453528576.00000\n",
      "Epoch: 110\n",
      "-------\n",
      "Looked at 0/268026 samples\n",
      "Looked at 128000/268026 samples\n",
      "Looked at 256000/268026 samples\n",
      "\n",
      "Train loss: 2999910656.00000 | Test loss: 2565708800.00000\n",
      "Epoch: 111\n",
      "-------\n",
      "Looked at 0/268026 samples\n",
      "Looked at 128000/268026 samples\n",
      "Looked at 256000/268026 samples\n",
      "\n",
      "Train loss: 2862012160.00000 | Test loss: 2565009664.00000\n",
      "Epoch: 112\n",
      "-------\n",
      "Looked at 0/268026 samples\n",
      "Looked at 128000/268026 samples\n",
      "Looked at 256000/268026 samples\n",
      "\n",
      "Train loss: 2849501952.00000 | Test loss: 2450823168.00000\n",
      "Epoch: 113\n",
      "-------\n",
      "Looked at 0/268026 samples\n",
      "Looked at 128000/268026 samples\n",
      "Looked at 256000/268026 samples\n",
      "\n",
      "Train loss: 2905533696.00000 | Test loss: 2624879360.00000\n",
      "Epoch: 114\n",
      "-------\n",
      "Looked at 0/268026 samples\n",
      "Looked at 128000/268026 samples\n",
      "Looked at 256000/268026 samples\n",
      "\n",
      "Train loss: 2907246592.00000 | Test loss: 2503148800.00000\n",
      "Epoch: 115\n",
      "-------\n",
      "Looked at 0/268026 samples\n",
      "Looked at 128000/268026 samples\n",
      "Looked at 256000/268026 samples\n",
      "\n",
      "Train loss: 2972716032.00000 | Test loss: 2550672896.00000\n",
      "Epoch: 116\n",
      "-------\n",
      "Looked at 0/268026 samples\n",
      "Looked at 128000/268026 samples\n",
      "Looked at 256000/268026 samples\n",
      "\n",
      "Train loss: 2887608832.00000 | Test loss: 2593267456.00000\n",
      "Epoch: 117\n",
      "-------\n",
      "Looked at 0/268026 samples\n",
      "Looked at 128000/268026 samples\n",
      "Looked at 256000/268026 samples\n",
      "\n",
      "Train loss: 2875488768.00000 | Test loss: 2641377024.00000\n",
      "Epoch: 118\n",
      "-------\n",
      "Looked at 0/268026 samples\n",
      "Looked at 128000/268026 samples\n",
      "Looked at 256000/268026 samples\n",
      "\n",
      "Train loss: 2776039936.00000 | Test loss: 2583966464.00000\n",
      "Epoch: 119\n",
      "-------\n",
      "Looked at 0/268026 samples\n",
      "Looked at 128000/268026 samples\n",
      "Looked at 256000/268026 samples\n",
      "\n",
      "Train loss: 2866992640.00000 | Test loss: 2704496640.00000\n",
      "Epoch: 120\n",
      "-------\n",
      "Looked at 0/268026 samples\n",
      "Looked at 128000/268026 samples\n",
      "Looked at 256000/268026 samples\n",
      "\n",
      "Train loss: 3098923008.00000 | Test loss: 2647555840.00000\n",
      "Epoch: 121\n",
      "-------\n",
      "Looked at 0/268026 samples\n",
      "Looked at 128000/268026 samples\n",
      "Looked at 256000/268026 samples\n",
      "\n",
      "Train loss: 2844589824.00000 | Test loss: 2838076672.00000\n",
      "Epoch: 122\n",
      "-------\n",
      "Looked at 0/268026 samples\n",
      "Looked at 128000/268026 samples\n",
      "Looked at 256000/268026 samples\n",
      "\n",
      "Train loss: 3039450880.00000 | Test loss: 2911500544.00000\n",
      "Epoch: 123\n",
      "-------\n",
      "Looked at 0/268026 samples\n",
      "Looked at 128000/268026 samples\n",
      "Looked at 256000/268026 samples\n",
      "\n",
      "Train loss: 2903020800.00000 | Test loss: 2652312064.00000\n",
      "Epoch: 124\n",
      "-------\n",
      "Looked at 0/268026 samples\n",
      "Looked at 128000/268026 samples\n",
      "Looked at 256000/268026 samples\n",
      "\n",
      "Train loss: 3006375680.00000 | Test loss: 2642085632.00000\n",
      "Train time on cuda:0: 542.286 seconds\n"
     ]
    }
   ],
   "source": [
    "# Import tqdm for progress bar\n",
    "from tqdm.auto import tqdm\n",
    "# Set the seed and start the timer\n",
    "train_time_start_on_cpu = timer()\n",
    "\n",
    "epochs = 125\n",
    "\n",
    "train_loss_list = []\n",
    "test_loss_list = []\n",
    "\n",
    "for epoch in tqdm(range(epochs)):\n",
    "    print(f\"Epoch: {epoch}\\n-------\")\n",
    "    ### Training\n",
    "    train_loss = 0\n",
    "    # Add a loop to loop through training batches\n",
    "    for batch, (X, y) in enumerate(train_dataloader):\n",
    "        model_0.train() \n",
    "        # 1. Forward pass\n",
    "        y_pred = model_0(X)\n",
    "\n",
    "        # 2. Calculate loss (per batch)\n",
    "        loss = loss_fn(y_pred, y)\n",
    "        train_loss += loss # accumulatively add up the loss per epoch \n",
    "\n",
    "        # 3. Optimizer zero grad\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # 4. Loss backward\n",
    "        loss.backward()\n",
    "        \n",
    "        torch.nn.utils.clip_grad_norm_(model_0.parameters(), 5)\n",
    "        \n",
    "        # 5. Optimizer step\n",
    "        optimizer.step()\n",
    "        \n",
    "        if batch % 2000 == 0:\n",
    "            # print(train_loss, loss)\n",
    "            print(f\"Looked at {batch * len(X)}/{len(train_dataloader.dataset)} samples\")\n",
    "\n",
    "    # Divide total train loss by length of train dataloader (average loss per batch per epoch)\n",
    "    \n",
    "    train_loss /= len(train_dataloader)\n",
    "    train_loss_list.append(train_loss)\n",
    "    test_loss = 0\n",
    "    model_0.eval()\n",
    "    with torch.inference_mode():\n",
    "        for X, y in test_dataloader:\n",
    "            # 1. Forward pass\n",
    "            test_pred = model_0(X)\n",
    "           \n",
    "            # 2. Calculate loss (accumatively)\n",
    "            test_loss += loss_fn(test_pred, y) # accumulatively add up the loss per epoch\n",
    "\n",
    "        \n",
    "        # Calculations on test metrics need to happen inside torch.inference_mode()\n",
    "        # Divide total test loss by length of test dataloader (per batch)\n",
    "        test_loss /= len(test_dataloader)\n",
    "        test_loss_list.append(test_loss)\n",
    "\n",
    "        # Divide total accuracy by length of test dataloader (per batch)\n",
    "\n",
    "    ## Print out what's happening\n",
    "    print(f\"\\nTrain loss: {train_loss:.5f} | Test loss: {test_loss:.5f}\")\n",
    "\n",
    "# Calculate training time      \n",
    "train_time_end_on_cpu = timer()\n",
    "total_train_time_model_0 = print_train_time(start=train_time_start_on_cpu, \n",
    "                                           end=train_time_end_on_cpu,\n",
    "                                           device=str(next(model_0.parameters()).device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "c3e8c38d-8323-4921-981d-b633c3b3589c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function matplotlib.pyplot.show(close=None, block=None)>"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for i, n in enumerate(train_loss_list):\n",
    "    train_loss_list[i] = n.detach().cpu().numpy()\n",
    "\n",
    "for i, n in enumerate(test_loss_list):\n",
    "    test_loss_list[i] = n.detach().cpu().numpy()\n",
    "    \n",
    "from matplotlib import pyplot as plt\n",
    "plt.plot(train_loss_list, label='train_loss')\n",
    "plt.plot(test_loss_list,label='val_loss')\n",
    "plt.legend()\n",
    "plt.show\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "159cff9b-37da-40b2-9d6a-fe2c6b5cf21b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model_name': 'RealEstatePredictor', 'model_loss': 2642085632.0}"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "def eval_model(model: torch.nn.Module, \n",
    "               data_loader: torch.utils.data.DataLoader, \n",
    "               loss_fn: torch.nn.Module):\n",
    "    \"\"\"Returns a dictionary containing the results of model predicting on data_loader.\n",
    "\n",
    "    Args:\n",
    "        model (torch.nn.Module): A PyTorch model capable of making predictions on data_loader.\n",
    "        data_loader (torch.utils.data.DataLoader): The target dataset to predict on.\n",
    "        loss_fn (torch.nn.Module): The loss function of model.\n",
    "        accuracy_fn: An accuracy function to compare the models predictions to the truth labels.\n",
    "\n",
    "    Returns:\n",
    "        (dict): Results of model making predictions on data_loader.\n",
    "    \"\"\"\n",
    "    predictions = []\n",
    "    actual = []\n",
    "    loss, acc = 0, 0\n",
    "    model.eval()\n",
    "    with torch.inference_mode():\n",
    "        for X, y in data_loader:\n",
    "            # Make predictions with the model\n",
    "            y_pred = model(X)\n",
    "            predictions.append(y_pred)\n",
    "            actual.append(y)\n",
    "            \n",
    "            # Accumulate the loss and accuracy values per batch\n",
    "            loss += loss_fn(y_pred, y)\n",
    "        \n",
    "        # Scale loss and acc to find the average loss/acc per batch\n",
    "        loss /= len(data_loader)        \n",
    "    return {\"model_name\": model.__class__.__name__, # only works when model was created with a class\n",
    "            \"model_loss\": loss.item()}, predictions, actual\n",
    "\n",
    "# Calculate model 0 results on test dataset\n",
    "model_0_results, predictions, actual = eval_model(model=model_0, data_loader=test_dataloader,\n",
    "    loss_fn=loss_fn\n",
    ")\n",
    "model_0_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "d16a8bac-ac6d-4f26-89b8-1051fa7f9ed1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2063"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "1a7f22d7-365e-4d4a-add0-daaca073bb79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2063"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(actual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "f7ca365c-71ec-465a-a88a-36eead7bd7fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "132014"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "8dd9fccb-8c1d-4a1c-9f02-978e0997b0f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "actual_output = []\n",
    "for raw in actual:\n",
    "    actual_output.extend(raw.flatten().tolist())\n",
    "predictions_output = [] \n",
    "for raw in predictions:\n",
    "    predictions_output.extend(raw.flatten().tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "ecdd8ac8-ae99-42f5-93ae-d70d5aa60736",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_actual = pd.DataFrame(actual_output,columns=['actual'])\n",
    "df_predictions = pd.DataFrame(predictions_output,columns=['predictions'])\n",
    "\n",
    "df_out = pd.concat([df_actual,df_predictions],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "08a0286c-34dc-4e5c-b5f9-0a8cec2d245c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>actual</th>\n",
       "      <th>predictions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>375000.0</td>\n",
       "      <td>368382.53125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>355000.0</td>\n",
       "      <td>332846.78125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>165500.0</td>\n",
       "      <td>171205.53125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>294500.0</td>\n",
       "      <td>294087.59375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>141500.0</td>\n",
       "      <td>146928.62500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132009</th>\n",
       "      <td>625000.0</td>\n",
       "      <td>623819.62500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132010</th>\n",
       "      <td>555000.0</td>\n",
       "      <td>539177.56250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132011</th>\n",
       "      <td>358000.0</td>\n",
       "      <td>361122.81250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132012</th>\n",
       "      <td>322500.0</td>\n",
       "      <td>331652.84375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132013</th>\n",
       "      <td>392000.0</td>\n",
       "      <td>414504.28125</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>132014 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          actual   predictions\n",
       "0       375000.0  368382.53125\n",
       "1       355000.0  332846.78125\n",
       "2       165500.0  171205.53125\n",
       "3       294500.0  294087.59375\n",
       "4       141500.0  146928.62500\n",
       "...          ...           ...\n",
       "132009  625000.0  623819.62500\n",
       "132010  555000.0  539177.56250\n",
       "132011  358000.0  361122.81250\n",
       "132012  322500.0  331652.84375\n",
       "132013  392000.0  414504.28125\n",
       "\n",
       "[132014 rows x 2 columns]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "daa016aa-3d75-4d5e-a26c-0ac413144519",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_out['diff'] = df_out.actual-df_out.predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "d945dea8-11c1-45af-b29a-18ebce897d4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>actual</th>\n",
       "      <th>predictions</th>\n",
       "      <th>diff</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>375000.0</td>\n",
       "      <td>368382.53125</td>\n",
       "      <td>6617.46875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>355000.0</td>\n",
       "      <td>332846.78125</td>\n",
       "      <td>22153.21875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>165500.0</td>\n",
       "      <td>171205.53125</td>\n",
       "      <td>-5705.53125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>294500.0</td>\n",
       "      <td>294087.59375</td>\n",
       "      <td>412.40625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>141500.0</td>\n",
       "      <td>146928.62500</td>\n",
       "      <td>-5428.62500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132009</th>\n",
       "      <td>625000.0</td>\n",
       "      <td>623819.62500</td>\n",
       "      <td>1180.37500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132010</th>\n",
       "      <td>555000.0</td>\n",
       "      <td>539177.56250</td>\n",
       "      <td>15822.43750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132011</th>\n",
       "      <td>358000.0</td>\n",
       "      <td>361122.81250</td>\n",
       "      <td>-3122.81250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132012</th>\n",
       "      <td>322500.0</td>\n",
       "      <td>331652.84375</td>\n",
       "      <td>-9152.84375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132013</th>\n",
       "      <td>392000.0</td>\n",
       "      <td>414504.28125</td>\n",
       "      <td>-22504.28125</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>132014 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          actual   predictions         diff\n",
       "0       375000.0  368382.53125   6617.46875\n",
       "1       355000.0  332846.78125  22153.21875\n",
       "2       165500.0  171205.53125  -5705.53125\n",
       "3       294500.0  294087.59375    412.40625\n",
       "4       141500.0  146928.62500  -5428.62500\n",
       "...          ...           ...          ...\n",
       "132009  625000.0  623819.62500   1180.37500\n",
       "132010  555000.0  539177.56250  15822.43750\n",
       "132011  358000.0  361122.81250  -3122.81250\n",
       "132012  322500.0  331652.84375  -9152.84375\n",
       "132013  392000.0  414504.28125 -22504.28125\n",
       "\n",
       "[132014 rows x 3 columns]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "cf7c114a-8a98-4fe3-a224-c628979a9850",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18096.410903618354"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "abs(df_out['diff']).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "7f6a637f-bafd-43bc-91be-7a9ae38b2ea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_out.to_csv('price_prediction.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e901c54-820d-429e-89cf-8c8263cfa86d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cc4820d-bd95-4c14-86c3-81451ef0d53f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
